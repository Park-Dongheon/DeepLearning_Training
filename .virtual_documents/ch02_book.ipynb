


import torch


import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline


dataset = pd.read_csv("data/car_evaluation.csv")


dataset.head()


categorical_columns = ["price", "maint", "doors", "persons", "lug_capacity", "safety"] # 임의로 나눠 놓음, ex) 폴더로 다 구분해 놓음


for category in categorical_columns:
    dataset[category] = dataset[category].astype("category")


price = dataset["price"].cat.codes.values
maint = dataset["maint"].cat.codes.values
doors = dataset["doors"].cat.codes.values
persons = dataset["persons"].cat.codes.values
lug_capacity = dataset["lug_capacity"].cat.codes.values
safety = dataset["safety"].cat.codes.values

categorical_data = np.stack([price, maint, doors, persons, lug_capacity, safety], 1)
categorical_data[:10]


categorical_data = torch.tensor(categorical_data, dtype=torch.int64)
categorical_data[:10]


outputs = pd.get_dummies(dataset.output)
outputs = outputs.values
outputs = torch.tensor(outputs).flatten()

print(categorical_data.shape)
print(outputs.shape)


categorical_column_sizes = [
    len(dataset[column].cat.categories) for column in categorical_columns
]


categorical_embedding_sizes = [
    (col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_column_sizes
]


print(categorical_embedding_sizes)


total_records = 1728
test_records = int(total_records * 0.2)

categorical_train_data = categorical_data[: total_records - test_records]
categorical_test_data = categorical_data[total_records - test_records : total_records]
train_outputs = outputs[: total_records - test_records]
test_outputs = outputs[total_records - test_records : total_records]


print(len(categorical_train_data)) # 훈련데이터
print(len(train_outputs))
print(len(categorical_test_data)) # 테스트 데이터
print(len(test_outputs))


class Model(nn.Module):
    def __init__(self, embedding_size, output_size, layers, p=0.4):
        super().__init__()
        self.all_embeddings = nn.ModuleList(
            [nn.Embedding(ni, nf) for ni, nf in embedding_size] # 레이어를 여러개 만듦
        )
        self.embedding_dropout = nn.Dropout(p)
        all_layers = []
        num_categorical_cols = sum((nf for ni, nf in embedding_size))
        input_size = num_categorical_cols
        for i in layers:
            all_layers.append(nn.Linear(input_size, i)) # 입력(input_size), 출력(i)
            all_layers.append(nn.ReLU(inplace=True)) # ReLU : 활성화 함수 - 입력으로 들어감, 출력값을 변화시키는 비선형 함수(기울기 소실 문제 해결방법)
            all_layers.append(nn.BatchNorm1d(i))
            all_layers.append(nn.Dropout(p)) # dropout : 과적합을 해결하기 위한 방법
            input_size = i
        all_layers.append(nn.Linear(layers[-1], output_size))
        self.layers = nn.Sequential(*all_layers) # 레이어들을 하나로 합침

    def forward(self, x_categorical): # 실제 GPU에서 계산되는 함수
        embeddings = []
        for i, e in enumerate(self.all_embeddings):
            embeddings.append(e(x_categorical[:, i]))
        x = torch.cat(embeddings, 1)
        x = self.embedding_dropout(x)
        x = self.layers(x)
        return x


model = Model(categorical_embedding_sizes, 4, [200, 100, 50], p=0.4) # 3개의 레이어가층을 돎
print(model)


loss_function = nn.CrossEntropyLoss() # 분류 문제 - 원-핫 인코딩 했을 때만 사용할 수 있는 오차 계산법
optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 아담 optimizer, lr(조금씩 내려가는 양)


if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")


epochs = 500
aggregated_losses = []
train_outputs = train_outputs.to(device=device, dtype=torch.int64)
for i in range(epochs):
    i += 1
    # 미니 배치 경사 하강법
    for X_train, y_label in categorical_train_data:
        y_pred = model(X_train)
        single_loss = loss_function(y_pred, train_outputs)
        aggregated_losses.append(single_loss)

    if i % 25 == 1:
        print(f"epoch: {i:3} loss: {single_loss.item():10.8f}")

    optimizer.zero_grad()
    single_loss.backward()
    optimizer.step()

print(f"epoch: {i:3} loss: {single_loss.item():10.10f}")


epochs = 500
aggregated_losses = []
train_outputs = train_outputs.to(device=device, dtype=torch.int64)
for i in range(epochs):
    i += 1
    y_pred = model(categorical_train_data)
    single_loss = loss_function(y_pred, train_outputs)
    aggregated_losses.append(single_loss)

    if i % 25 == 1:
        print(f"epoch: {i:3} loss: {single_loss.item():10.8f}")

    optimizer.zero_grad()
    single_loss.backward()
    optimizer.step()

print(f"epoch: {i:3} loss: {single_loss.item():10.10f}")


test_outputs = test_outputs.to(device=device, dtype=torch.int64)
with torch.no_grad():
    y_val = model(categorical_test_data)
    loss = loss_function(y_val, test_outputs)
print(f"Loss: {loss:.8f}")


print(y_val[:5])


y_val = np.argmax(y_val, axis=1)
print(y_val[:5])


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(test_outputs, y_val))
print(classification_report(test_outputs, y_val))
print(accuracy_score(test_outputs, y_val))



