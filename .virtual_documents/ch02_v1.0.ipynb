import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn

from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader


def preprocessing():
    dataset = pd.read_csv("data/car_evaluation.csv")
    
    categorical_columns = ["price", "maint", "doors", "persons", "lug_capacity", "safety"]
    for category in categorical_columns:
        dataset[category] = dataset[category].astype("category")

    price = dataset["price"].cat.codes.values
    maint = dataset["maint"].cat.codes.values
    doors = dataset["doors"].cat.codes.values
    persons = dataset["persons"].cat.codes.values
    lug_capacity = dataset["lug_capacity"].cat.codes.values
    safety = dataset["safety"].cat.codes.values
    
    categorical_data =  np.stack([price, maint, doors, persons, lug_capacity, safety],1)
    categorical_data = torch.tensor(categorical_data, dtype=torch.int64)
    
    categorical_colum_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]
    categorical_embedding_sizes = [(col_size, min(50, (col_size)+1) // 2) for col_size in categorical_colum_sizes]

    outputs = dataset["output"].astype("category").cat.codes.values
    outputs = torch.tensor(outputs, dtype=torch.int64).flatten()

    return categorical_data, outputs, categorical_embedding_sizes
    
categorical_data, outputs, categorical_embedding_sizes = preprocessing()


X_train, X_test, y_train, y_test = train_test_split(categorical_data,
                                                    outputs,
                                                    test_size = 0.2,
                                                    random_state=42)


class CarEvalutationDataset(Dataset):
    def __init__(self, categorical_data, outputs):
        self.categorical_data = categorical_data
        self.outputs = outputs

    def __len__(self):
        return len(self.categorical_data)

    def __getitem__(self, idx):
        X = self.categorical_data[idx]
        y = self.outputs[idx]
        return X, y


train_dataset = CarEvalutationDataset(X_train, y_train)
test_dataset = CarEvalutationDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)


class Model(nn.Module):
    def __init__(self, embedding_size, output_size, layers, p=0.4):
        super().__init__()
        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])
        self.embedding_dropout = nn.Dropout(p)
        all_layers = []
        num_categorical_cols = sum((nf for ni, nf in embedding_size))
        input_size = num_categorical_cols
        for i in layers:
            all_layers.append(nn.Linear(input_size, i))
            all_layers.append(nn.ReLU(inplace=True))
            all_layers.append(nn.BatchNorm1d(i))
            all_layers.append(nn.Dropout(p))
            input_size = i
        all_layers.append(nn.Linear(layers[-1], output_size))
        self.layers = nn.Sequential(*all_layers)

    def forward(self, x_categorical):
        embeddings = []
        for i, e in enumerate(self.all_embeddings):
            embeddings.append(e(x_categorical[:,i]))
    
        x = torch.cat(embeddings, 1)
        x = self.embedding_dropout(x)
        x = self.layers(x)
        return x          


model = Model(categorical_embedding_sizes, 4, [200, 100, 50], p=0.4)
model


loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


epochs = 500
aggregated_losses = []
for epoch in range(epochs):
    model.train()
    for x_batch, y_batch in train_loader:
        y_pred = model(x_batch)
        single_loss = loss_function(y_pred, y_batch)
        aggregated_losses.append(single_loss.item())

        optimizer.zero_grad()
        single_loss.backward()
        optimizer.step()

    if (epoch + 1) % 25 == 0:
        print(single_loss.item())



