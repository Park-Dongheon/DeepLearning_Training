


import os
import io
import requests
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms.v2 as v2
from torchsummary import summary
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torchvision.utils import make_grid
from torchvision.datasets import ImageFolder
from PIL import Image
from pathlib import Path





train_transform = v2.Compose([
    v2.Resize((256, 256)),
    v2.RandomResizedCrop(size=(224, 224), antialias=True),
    v2.RandomHorizontalFlip(p=0.5),
    v2.RandomVerticalFlip(p=0.5),
    v2.ToTensor(),
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = v2.Compose([
    v2.Resize((256, 256)),
    v2.CenterCrop(size=(224, 224)),
    v2.ToTensor(),
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])


train_path = 'data/catanddog/train'
train_dataset = ImageFolder(train_path, transform=train_transform)

test_path = 'data/catanddog/test'
test_dataset = ImageFolder(test_path, transform=test_transform)


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)


from torchvision.models import resnet50, ResNet50_Weights


model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
model


summary(model, (3, 224, 224))


# for p in model.parameters():
#     p.requires_grad = False


summary(model, (3, 224, 224))


model.fc = nn.Sequential(
    nn.Linear(2048, 512),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(512, 32),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(32, 2),
)
model


torch.__version__, torch.cuda.is_available()


def train(model, data_loader, loss_fn, optimizer, EPOCHS=1):
    loss_history = []
    acc_history = []
    epoch_loss = 0.
    num_corrects = 0
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)

    for n in range(EPOCHS):
        step_loss = 0
        step_loss_his = []
        num_corrects = 0
        for idx, (X_train, y_label) in enumerate(data_loader):
            inputs = X_train.to(device)
            labels = y_label.to(device)
            
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()
            
            step_loss += loss.item()
            step_loss_his.append(step_loss)
            # print(torch.argmax(outputs, dim=1))
            # print(y_label)
            train_acc = torch.sum(torch.argmax(outputs, dim=1) == labels)
            num_corrects += train_acc

            # if idx % 5 == 0:
            # epoch_loss = step_loss / len(X_train) # loss per batch
            # print(' batch {} loss: {}, acc: {}'.format(idx + 1, step_loss, train_acc/len(X_train)))
            # step_loss = 0

        epoch_avg_loss = sum(step_loss_his) / len(data_loader)
        loss_history.append(epoch_avg_loss)
        epoch_loss = step_loss / len(data_loader)
        accuracy = num_corrects / len(data_loader.dataset)
        print('EPOCH {}, loss: {:.4f}, acc: {:.4f}'.format(n + 1, epoch_loss, accuracy))
    # return modle


# 교재(p.216)
def train_model(model, data_loader, loss_fn, optimizer, EPOCHS=13):
    since = time.time() # 컴퓨터의 현재 시각을 구하는 함수
    acc_history = []
    loss_history = []
    best_acc = 0.0
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    for EPOCH in range(EPOCHS): # EPOCHS(13)만큼 반복
        print('EPOCH {}/{}'.format(EPOCH, EPOCHS - 1))
        print('-' * 10)

        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in data_loader: # 데이터로더에 전달된 데이터만큼 반복
            inputs = inputs.to(device)
            labels = labels.to(device)
            model.to(device)

            optimizer.zero_grad # 기울기를 0으로 설정
            outputs = model(inputs) # 순전파 학습
            loss = loss_fn(outputs, labels)
            loss.backward() # 역전파 학습
            optimizer.step()

            running_loss += loss.item() * inputs.size(0) # 출력 결과와 레이블의 오차를 계산한 결과를 누적하여 저장
            running_corrects += torch.sum(preds == labels.data) # 출력 결과와 레이블이 동일한지 확인한 결과를 누적하여 저장

        epoch_loss = running_loss / len(data_loader.dataset) # 평균 오차 계산
        epoch_acc = running_corrects.double() / len(data_loader.dataset) # 평균 정확도 계산
        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))

        if epoch_acc > best_acc:
            best_acc = epoch_acc

        acc_history.append(epoch_acc.item())
        loss_history.append(epoch_loss)
        torch.save(model.state_dict(), os.path.join('./data/catanddog/', '{0:0=2d}.pth'.format(epoch))) # 모델 재사용을 위해 저장해 둡니다.
        print()

    time_elapsed = time.time() - since # 실행 시간(학습 시간)을 계산
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best Acc: {:4f}'.format(best_acc))
    return acc_history, loss_history # 모델의 정확도와 오차를 반환
            
            


loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())


train(model, train_loader, loss_fn, optimizer, 5)


from PIL import Image

image = Image.open('./data/dogs-vs-cats/Cat/cat.1.jpg')
to_tensor = v2.Compose([
            v2.Resize((224, 224)),
            v2.ToTensor()
])
tensor_image = to_tensor(image)
tensor_image[None, :, :, :].shape
tensor_image = torch.unsqueeze(tensor_image, dim=0)
tensor_image.shape


model(tensor_image)





from torchvision.io.image import read_image
from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image

img = read_image("./grace_hopper_517x606.jpg")

# Step 1: Initialize model with the best available weights
weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT
model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)
model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = [preprocess(img)]

# Step 4: Use the model and visualize the prediction
prediction = model(batch)[0]
labels = [weights.meta["categories"][i] for i in prediction["labels"]]
box = draw_bounding_boxes(img, boxes=prediction["boxes"],
                          labels=labels,
                          colors="red",
                          width=4, font_size=30)
im = to_pil_image(box.detach())
im.show()


batch = [preprocess(img)]
batch[0].shape


model


from PIL import Image
from torchvision.io import read_image

img_path = '/data/PennFudanPed/PNGImages/FudanPed00046.png'
image = Image.open(img_path)
image


import os
import torch

from torchvision.io import read_image
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors
from torchvision.transforms.v2 import functional as F


class PennFudanDataset(torch.utils.data.Dataset):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(root, "PNGImages"))))
        self.masks = list(sorted(os.listdir(os.path.join(root, "PedMasks"))))

    def __getitem__(self, idx):
        # load images and masks
        img_path = os.path.join(self.root, "PNGImages", self.imgs[idx])
        mask_path = os.path.join(self.root, "PedMasks", self.masks[idx])
        img = read_image(img_path)
        mask = read_image(mask_path)
        # instances are encoded as different colors
        obj_ids = torch.unique(mask)
        # first id is the background, so remove it
        obj_ids = obj_ids[1:]
        num_objs = len(obj_ids)

        # split the color-encoded mask into a set
        # of binary masks
        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)

        # get bounding box coordinates for each mask
        boxes = masks_to_boxes(masks)

        # there is only one class
        labels = torch.ones((num_objs,), dtype=torch.int64)

        image_id = idx
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        # suppose all instances are not crowd
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

        # Wrap sample and targets into torchvision tv_tensors:
        img = tv_tensors.Image(img)

        target = {}
        target["boxes"] = tv_tensors.BoundingBoxes(boxes, format="XYXY", canvas_size=F.get_size(img))
        target["masks"] = tv_tensors.Mask(masks)
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.imgs)


my_transform = v2.Compose([
    v2.ToTensor()
])
root = './data/PennFudanPed'
dataset = ImageFolder(root, transform=my_transform)


import utils

from torch.utils.data import DataLoader
data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)


import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# load a model pre-trained on COCO
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights="DEFAULT")

# replace the classifier with a new one, that has
# num_classes which is user-defined
num_classes = 2  # 1 class (person) + background
# get number of input features for the classifier
in_features = model.roi_heads.box_predictor.cls_score.in_features
# replace the pre-trained head with a new one
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)


model


import matplotlib.pyplot as plt
from torchvision.io import read_image


image = read_image("data/PennFudanPed/PNGImages/FudanPed00046.png")
mask = read_image("data/PennFudanPed/PedMasks/FudanPed00046_mask.png")

plt.figure(figsize=(16, 8))
plt.subplot(121)
plt.title("Image")
plt.imshow(image.permute(1, 2, 0))
plt.subplot(122)
plt.title("Mask")
plt.imshow(mask.permute(1, 2, 0))



