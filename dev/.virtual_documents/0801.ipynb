import os
import io
import requests
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms.v2 as v2
from torchsummary import summary
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torchvision.utils import make_grid
from torchvision.datasets import ImageFolder
from torchvision.datasets import MNIST, FashionMNIST, CIFAR10
from torchvision.models import resnet50, ResNet50_Weights
from PIL import Image
from pathlib import Path


class MyDataset():
    def __init__(self, root='./data/dogs-vs-cats', transform=None):
        self.root = root
        self.image_paths = []
        self.__classes = {'Cat':0, 'Dog':1}
        self.labels = []

        for dname in os.listdir(self.root):
            print(type(dname))
            print(os.path.isdir(Path(root, dname)))
            new_path = Path(root, dname)
            if os.path.isdir(new_path):
                for file in os.listdir(new_path):
                    # print(file)
                    self.image_paths.append(str(Path(new_path, file)))
                    self.labels.append(self.__classes[dname])

        self.transform = transform

    def __getitem__(self, idx):
        path = self.image_paths[idx]
        # print(path)
        image = Image.open(path)
        print(self.transform(image).shape)
        return self.transform(image), self.labels[idx]

    def __len__(self):
        return len(self.labels)


transform = v2.Compose([
            v2.RandomResizedCrop(size=(224, 224), antialias=True),
            v2.ToTensor(),
            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
dataset = MyDataset('./data/dogs-vs-cats', transform)
data, label = dataset[0]
data.shape, label


data_loader = DataLoader(dataset, batch_size=32, shuffle=True)


for X_train, y_label in data_loader:
    print(X_train.shape, y_label.shape)
    break


class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 6, 5, padding='same'),
            nn.ReLU(),
            nn.Dropout(),
            nn.MaxPool2d(2),
            nn.Conv2d(6, 16, 5, padding='same'),
            nn.ReLU(),
            nn.Dropout(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 5, padding='same'),
            nn.ReLU(),
            nn.Dropout(),
            nn.MaxPool2d(2),
        )
        self.flatten = nn.Flatten()
        self.classifier = nn.Sequential(
            nn.Linear(32 * 28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.flatten(x)
        x = self.classifier(x)
        return x


loss_fn = nn.CrossEntropyLoss()


model = LeNet5()
color_image = torch.rand(32, 3, 224, 224)
model(color_image).shape


for X_train, y_label in data_loader:
    outputs = model(X_train)
    print(outputs.shape)
    break


loss_fn = nn.CrossEntropyLoss()


summary(model, (3, 224, 224))


optimizer = optim.Adam(model.parameters(), lr=0.0001)

for X_train, y_label in data_loader:
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = loss_fn(outputs, y_label)
    loss.backward()
    optimizer.step()
    break


EPOCHS = 1
for n in range(EPOCHS):
    print('EPOCHS {}:'.format(n + 1))
    epoch_loss = 0
    step_loss = 0
    for idx, (X_train, y_label) in enumerate(data_loader):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = loss_fn(outputs, y_label)
        loss.backward()
        optimizer.step()

        step_loss += loss.item()
        print(' batch {} loss: {}'.format(idx + 1, step_loss))
        step_loss = 0


train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])
print(len(train_dataset), len(test_dataset))


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)

for X_train, y_label in train_loader:
    print(X_train.shape, y_label.shape)
    break


EPOCHS = 1

for n in range(EPOCHS):
    print('EPOCHS {}'.format(n + 1))
    epoch_loss = 0
    step_loss = 0
    for idx, (X_train, y_label) in enumerate(data_loader):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = loss_fn(outputs, y_label)
        loss.backward()
        optimizer.step()

        step_loss += loss.item()
        train_acc = torch.sum(torch.argmax(outputs, dim=1) == y_label)
        print(' batch {} loss: {}'.format(idx + 1, step_loss))
        step_loss = 0


from torchvision.models import alexnet, AlexNet_Weights


model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)
model


for p in model.parameters():
    print(p.names, type(p), p.shape, p.requires_grad)
    p.reqrires_grad =False


summary(model, (3, 224, 224))


model.classifier = nn.Sequential(
    nn.Dropout(),
    nn.Linear(9216, 512),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(512, 64),
    nn.ReLU(),
    nn.Linear(64, 2),
    nn.Softmax(dim=1)
)
model


summary(model, (3, 224, 224))


image = torch.rand(32, 3, 224, 224)
model(image)


for p in model.parameters():
    print(p.names, type(p), p.shape, p.requires_grad)


my_transform = v2.Compose([
    v2.Resize(size=(224, 224)),
    v2.ToTensor(),    
])

root = './data/dogs-vs-cats'
dataset = ImageFolder(root, transform=my_transform)


data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

for X_train, y_label in data_loader:
    print(X_train.shape, y_label.shape)
    break


loss_fn = nn.CrossEntropyLoss()


optimizer = optim.Adam(model.parameters())


def fit(model, data_loader, loss_fn, optimizer, epochs):
    for i in range(epochs):
        for X_train, y_label in data_loader:
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = loss_fn(outputs, y_label)
            loss.backward()
            optimizer.step()
    return model


fit(model, data_loader, loss_fn, optimizer, 1)


for X_train, y_label in data_loader:
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = loss_fn(outputs, y_label)
    loss.backward()
    optimizer.step()
    break


from torchvision.models import vgg16, VGG16_Weights

model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)
model


summary(model, (3, 224, 224))


for p in model.parameters():
    print(p.names, type(p), p.shape, p.requires_grad)
    p.reqrires_grad =False


model.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 1024),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(1024, 2),
)


summary(model, (3, 224, 224))


from torchvision.models import resnet50, ResNet50_Weights

model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
model


# for p in model.parameters():
#     print(p.names, type(p), p.shape, p.requires_grad)
#     p.reqrires_grad =False


model.fc = nn.Sequential(
    nn.Linear(2048, 512),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(512, 32),
    nn.ReLU(),
    nn.Dropout(),
    nn.Linear(32, 2)
)
model


summary(model, (3, 224, 224))


model = resnet50(pretrained=True)
image = torch.rand(32, 3, 224, 224)
model(image)


my_transform = v2.Compose([
               v2.RandomResizedCrop(size=(224, 224), antialias=True),
               v2.ToTensor()
])

root = './data/catanddog'
dataset = ImageFolder(root, transform=my_transform)
image, label = dataset[0]
type(image), type(label)


data_loader = DataLoader(dataset, batch_size=32)

for X_train, y_label in data_loader:
    outputs = model(X_train)
    print(X_train.shape, y_label.shape)
    print(outputs.shape)
    break


image_paths = []
labels = []

for dname in os.listdir(root):
    print(dname)
    print(type(dname))
    print(os.path.isdir(Path(root, dname)))
    new_path = Path(root, dname)
    if os.path.isdir(new_path):
        for file in os.listdir(new_path):
            print(file)
            image_paths.append(str(Path(new_path, file)))
            labels.append(dname)

print(len(image_paths), len(labels), labels.count('Cat'), labels.count('Dog'))


loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())


def train_model(model, data_loader, loss_fn, optimizer, device, num_epochs=13, is_train=True):
    since = time.time()
    acc_history = []
    loss_history = []
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('EPOCH {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in data_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            model.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            _, preds = torch.max(outputs, 1)
            loss.backward()
            optimiser.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(data_loader.dataset)
        epoch_acc = running_corrects.double() / len(data_loader.dataset)

        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))

        if epoch_acc > best_acc:
            best_acc = epoch_acc

        acc_history.append(epoch_acc.item())
        loss_history.append(epoch_loss)
        torch.save(model.state_dict(), os.path.join('./data/dogs-vs-cats', '{0.0=2d}.pth'.format(epoch)))
        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best Acc: {:4f}'.format(best_acc))
    return acc_history, loss_history


params_to_update = []
for name, param in resnet50.named_parameters():
    if param.requires_grad == True:
        params_to_update.append(param)
        print("\t", name)

optimizer = optim.Adam(params_to_update)


'cuda' if
