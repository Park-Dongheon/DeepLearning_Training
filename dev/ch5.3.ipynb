{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478538f1-1c28-4edc-805e-2acc32fd22bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c16d0a-e9bd-4e93-b6e2-b3282578e240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1bd0ed-bdc2-4700-8a68-d1d2cafc1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57a2b67-2f49-43d5-88ac-29863d500b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f919a0-7652-492e-9fb5-345feec0c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    #print(p.names, type(p), p.shape, p.requires_grad)\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af790fba-bfaa-436a-91c2-d4f19cca0a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 256, 6, 6]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 55, 55]          (23,296)\n",
      "|    └─ReLU: 2-2                         [-1, 64, 55, 55]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 64, 27, 27]          --\n",
      "|    └─Conv2d: 2-4                       [-1, 192, 27, 27]         (307,392)\n",
      "|    └─ReLU: 2-5                         [-1, 192, 27, 27]         --\n",
      "|    └─MaxPool2d: 2-6                    [-1, 192, 13, 13]         --\n",
      "|    └─Conv2d: 2-7                       [-1, 384, 13, 13]         (663,936)\n",
      "|    └─ReLU: 2-8                         [-1, 384, 13, 13]         --\n",
      "|    └─Conv2d: 2-9                       [-1, 256, 13, 13]         (884,992)\n",
      "|    └─ReLU: 2-10                        [-1, 256, 13, 13]         --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 13, 13]         (590,080)\n",
      "|    └─ReLU: 2-12                        [-1, 256, 13, 13]         --\n",
      "|    └─MaxPool2d: 2-13                   [-1, 256, 6, 6]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [-1, 256, 6, 6]           --\n",
      "├─Sequential: 1-3                        [-1, 1000]                --\n",
      "|    └─Dropout: 2-14                     [-1, 9216]                --\n",
      "|    └─Linear: 2-15                      [-1, 4096]                (37,752,832)\n",
      "|    └─ReLU: 2-16                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-17                     [-1, 4096]                --\n",
      "|    └─Linear: 2-18                      [-1, 4096]                (16,781,312)\n",
      "|    └─ReLU: 2-19                        [-1, 4096]                --\n",
      "|    └─Linear: 2-20                      [-1, 1000]                (4,097,000)\n",
      "==========================================================================================\n",
      "Total params: 61,100,840\n",
      "Trainable params: 0\n",
      "Non-trainable params: 61,100,840\n",
      "Total mult-adds (M): 775.28\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3.77\n",
      "Params size (MB): 233.08\n",
      "Estimated Total Size (MB): 237.43\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 256, 6, 6]           --\n",
       "|    └─Conv2d: 2-1                       [-1, 64, 55, 55]          (23,296)\n",
       "|    └─ReLU: 2-2                         [-1, 64, 55, 55]          --\n",
       "|    └─MaxPool2d: 2-3                    [-1, 64, 27, 27]          --\n",
       "|    └─Conv2d: 2-4                       [-1, 192, 27, 27]         (307,392)\n",
       "|    └─ReLU: 2-5                         [-1, 192, 27, 27]         --\n",
       "|    └─MaxPool2d: 2-6                    [-1, 192, 13, 13]         --\n",
       "|    └─Conv2d: 2-7                       [-1, 384, 13, 13]         (663,936)\n",
       "|    └─ReLU: 2-8                         [-1, 384, 13, 13]         --\n",
       "|    └─Conv2d: 2-9                       [-1, 256, 13, 13]         (884,992)\n",
       "|    └─ReLU: 2-10                        [-1, 256, 13, 13]         --\n",
       "|    └─Conv2d: 2-11                      [-1, 256, 13, 13]         (590,080)\n",
       "|    └─ReLU: 2-12                        [-1, 256, 13, 13]         --\n",
       "|    └─MaxPool2d: 2-13                   [-1, 256, 6, 6]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [-1, 256, 6, 6]           --\n",
       "├─Sequential: 1-3                        [-1, 1000]                --\n",
       "|    └─Dropout: 2-14                     [-1, 9216]                --\n",
       "|    └─Linear: 2-15                      [-1, 4096]                (37,752,832)\n",
       "|    └─ReLU: 2-16                        [-1, 4096]                --\n",
       "|    └─Dropout: 2-17                     [-1, 4096]                --\n",
       "|    └─Linear: 2-18                      [-1, 4096]                (16,781,312)\n",
       "|    └─ReLU: 2-19                        [-1, 4096]                --\n",
       "|    └─Linear: 2-20                      [-1, 1000]                (4,097,000)\n",
       "==========================================================================================\n",
       "Total params: 61,100,840\n",
       "Trainable params: 0\n",
       "Non-trainable params: 61,100,840\n",
       "Total mult-adds (M): 775.28\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 3.77\n",
       "Params size (MB): 233.08\n",
       "Estimated Total Size (MB): 237.43\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67378019-2823-43d1-9105-7bff2674bfbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (4): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(9216, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c4fa71-8440-4112-a816-f456b1e89f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in c:\\users\\user\\anaconda3\\envs\\torch-book\\lib\\site-packages (1.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1283ea61-a40b-4dda-b9f7-b0ef703878a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 256, 6, 6]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 55, 55]          (23,296)\n",
      "|    └─ReLU: 2-2                         [-1, 64, 55, 55]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 64, 27, 27]          --\n",
      "|    └─Conv2d: 2-4                       [-1, 192, 27, 27]         (307,392)\n",
      "|    └─ReLU: 2-5                         [-1, 192, 27, 27]         --\n",
      "|    └─MaxPool2d: 2-6                    [-1, 192, 13, 13]         --\n",
      "|    └─Conv2d: 2-7                       [-1, 384, 13, 13]         (663,936)\n",
      "|    └─ReLU: 2-8                         [-1, 384, 13, 13]         --\n",
      "|    └─Conv2d: 2-9                       [-1, 256, 13, 13]         (884,992)\n",
      "|    └─ReLU: 2-10                        [-1, 256, 13, 13]         --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 13, 13]         (590,080)\n",
      "|    └─ReLU: 2-12                        [-1, 256, 13, 13]         --\n",
      "|    └─MaxPool2d: 2-13                   [-1, 256, 6, 6]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [-1, 256, 6, 6]           --\n",
      "├─Sequential: 1-3                        [-1, 2]                   --\n",
      "|    └─Dropout: 2-14                     [-1, 9216]                --\n",
      "|    └─Linear: 2-15                      [-1, 64]                  589,888\n",
      "|    └─ReLU: 2-16                        [-1, 64]                  --\n",
      "|    └─Linear: 2-17                      [-1, 2]                   130\n",
      "|    └─Softmax: 2-18                     [-1, 2]                   --\n",
      "==========================================================================================\n",
      "Total params: 3,059,714\n",
      "Trainable params: 590,018\n",
      "Non-trainable params: 2,469,696\n",
      "Total mult-adds (M): 659.21\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3.70\n",
      "Params size (MB): 11.67\n",
      "Estimated Total Size (MB): 15.95\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 256, 6, 6]           --\n",
       "|    └─Conv2d: 2-1                       [-1, 64, 55, 55]          (23,296)\n",
       "|    └─ReLU: 2-2                         [-1, 64, 55, 55]          --\n",
       "|    └─MaxPool2d: 2-3                    [-1, 64, 27, 27]          --\n",
       "|    └─Conv2d: 2-4                       [-1, 192, 27, 27]         (307,392)\n",
       "|    └─ReLU: 2-5                         [-1, 192, 27, 27]         --\n",
       "|    └─MaxPool2d: 2-6                    [-1, 192, 13, 13]         --\n",
       "|    └─Conv2d: 2-7                       [-1, 384, 13, 13]         (663,936)\n",
       "|    └─ReLU: 2-8                         [-1, 384, 13, 13]         --\n",
       "|    └─Conv2d: 2-9                       [-1, 256, 13, 13]         (884,992)\n",
       "|    └─ReLU: 2-10                        [-1, 256, 13, 13]         --\n",
       "|    └─Conv2d: 2-11                      [-1, 256, 13, 13]         (590,080)\n",
       "|    └─ReLU: 2-12                        [-1, 256, 13, 13]         --\n",
       "|    └─MaxPool2d: 2-13                   [-1, 256, 6, 6]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [-1, 256, 6, 6]           --\n",
       "├─Sequential: 1-3                        [-1, 2]                   --\n",
       "|    └─Dropout: 2-14                     [-1, 9216]                --\n",
       "|    └─Linear: 2-15                      [-1, 64]                  589,888\n",
       "|    └─ReLU: 2-16                        [-1, 64]                  --\n",
       "|    └─Linear: 2-17                      [-1, 2]                   130\n",
       "|    └─Softmax: 2-18                     [-1, 2]                   --\n",
       "==========================================================================================\n",
       "Total params: 3,059,714\n",
       "Trainable params: 590,018\n",
       "Non-trainable params: 2,469,696\n",
       "Total mult-adds (M): 659.21\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 3.70\n",
       "Params size (MB): 11.67\n",
       "Estimated Total Size (MB): 15.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9715002-047d-4335-bc9e-6a6be0c78e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(32, 3, 224, 224)\n",
    "model(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6dedf57-4896-45d5-b7ae-5e30c1827e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([64, 3, 11, 11]) False\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([64]) False\n",
      "(None, None, None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([192, 64, 5, 5]) False\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([192]) False\n",
      "(None, None, None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([384, 192, 3, 3]) False\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([384]) False\n",
      "(None, None, None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([256, 384, 3, 3]) False\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([256]) False\n",
      "(None, None, None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 3, 3]) False\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([256]) False\n",
      "(None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([64, 9216]) True\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([64]) True\n",
      "(None, None) <class 'torch.nn.parameter.Parameter'> torch.Size([2, 64]) True\n",
      "(None,) <class 'torch.nn.parameter.Parameter'> torch.Size([2]) True\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.names, type(p), p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "004c9e83-10b7-43fb-9264-4c00427526e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d020655-daaa-4faf-8232-e892681ec651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aab9af1-2abb-43c2-b53b-f7496f84187e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "root = 'data/dogs-vs-cats'\n",
    "dataset = ImageFolder(root, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140c376d-195f-41e7-8c2e-b02341d82756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31630882-9a3c-4533-947b-157662bde519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ab6eac-017a-483d-9aea-a44eeaddb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dcf3e1d-ae77-40d8-8cce-84702cc25c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, data_loader, loss_fn, optimizer, epochs):\n",
    "    for i in range(epochs):\n",
    "        for X_train, y_label in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = loss_fn(outputs, y_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d49010-21df-4328-9a11-fb1e66aa8427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for X_train, y_label in data_loader:\n",
    "    print(X_train.shape, y_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f90acfa6-ced2-4d11-8bc5-bc0caeb6e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(model, data_loader, loss_fn, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bbe0d2b-5634-4e9e-b472-5f50e2dfd323",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[1;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m(outputs, y_label)\n\u001b[0;32m      5\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "for X_train, y_label in data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = loss_fn(outputs, y_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00ed19-633c-46ab-9b1e-69e938728d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cc6ea-8cab-46db-ae5b-ab2b1ad11c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df12b1-1d43-49a4-8c84-f84805b34ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1a142-a682-44df-ac52-9576a072b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d953a-15d1-48ee-b7b7-83629a445e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 512),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(512, 32),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(32, 2),\n",
    "    )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827650d-0365-48df-af62-a4145c475034",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = v2.Compose([\n",
    "    v2.Resize((256, 256)),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize((256, 256)),\n",
    "    v2.CenterCrop(size=(224, 224)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627d3c9-94d0-48df-88f9-c934e852b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/catanddog/train'\n",
    "train_dataset = ImageFolder(train_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77e041-d051-4330-ae6a-243a46d68f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/catanddog/test'\n",
    "test_dataset = ImageFolder(test_path, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36badf-9836-49db-92db-010014844ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80ecc7-8b4c-474b-a444-4d95a0a3b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae233ef-1a66-4c7b-b449-17ad9d7ef668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_fn, optimizer, EPOCHS=1):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    epoch_loss = 0.\n",
    "    num_corrects = 0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    for n in range(EPOCHS):\n",
    "        step_loss = 0\n",
    "        num_corrects = 0\n",
    "        for idx, (X_train, y_label) in enumerate(data_loader):\n",
    "            inputs = X_train.to(device)\n",
    "            labels = y_label.to(device)\n",
    "            model = model.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step_loss += loss.item()\n",
    "            #print(torch.argmax(outputs, dim=1))\n",
    "            #print(y_label)\n",
    "            train_acc = torch.sum(torch.argmax(outputs, dim=1) == labels)\n",
    "            num_corrects += train_acc\n",
    "            #if idx % 5 == 0:\n",
    "            #epoch_loss = step_loss / len(X_train) # loss per batch\n",
    "            #print('  batch {} loss: {}, acc: {}'.format(idx + 1, step_loss, train_acc/len(X_train)))\n",
    "            #step_loss = 0\n",
    "        epoch_loss = step_loss / len(data_loader)\n",
    "        accuracy = num_corrects / len(data_loader.dataset)\n",
    "        print('EPOCH {}, loss: {}, acc: {}'.format(n + 1, epoch_loss, accuracy))\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b9fd0-d81a-4ae5-984f-6cf714fa71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bba35a-6d90-426a-a9ad-bb3472ff5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, loss_fn, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9727a5-fee0-4e3c-9384-5a75325c9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9847e5-b71f-4b05-9991-bce685036687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d5ff3-ac74-4d65-a836-25ec2d07dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef7162-9248-4e6a-ac45-1d437fca5e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93549c-e6ea-4a44-8f4b-9c0899383fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d72fb-b5ea-4759-9a69-b284b084a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.classifier), type(model.classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08efe98-54ec-42e2-84c9-591177e40ae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(type(p), p.shape, p.requires_grad)\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d5c7a-ec84-4217-9cc9-d2d93b1ad7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(type(p), p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d08275-3d00-4872-b5fd-a835e82ab5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.classifier.parameters():\n",
    "    print(type(p), p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b2d24-6529-4f0e-a3b4-cfa9bb287854",
   "metadata": {},
   "outputs": [],
   "source": [
    "256 * 6 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fba5fe-7558-4d66-899f-ee03d470c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(9216, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118c0fd-7953-406a-84dd-bfd16c681821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252742b-096e-4ba4-85e0-5014059bb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.rand(32, 3, 224, 224)\n",
    "model(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc46d7d-2c60-4b63-9ef6-62f4f2cb190b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cea47e-b10c-4b78-a095-94a8b2837617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa28f6-7878-466f-b68c-b0bd82bdef87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca2e7d-5750-4313-a947-b7cab1c759d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 512),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(512, 64),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(64, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bb3d2-0127-4e20-9f34-091b147885e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc29db-ec91-4f89-a1dc-f3f13614fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    #print(type(p), p.shape, p.requires_grad)\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc71f96-9244-407a-88eb-cb0148e2774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "512 *  7 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72711c6-cc3b-4802-afd5-28d93fb08348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(1024, 2)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d14f8-a3f5-48b3-b705-a496819aefa5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image = torch.rand(32, 3, 224, 224)\n",
    "model(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e16c9-7bfa-4002-82a5-ea415a05d55f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8318b-184d-4c70-8a81-d7f3c3001519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "image = Image.open('./data/dogs-vs-cats/Cat/cat.1.jpg')\n",
    "to_tensor = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToTensor()])\n",
    "tensor_image = to_tensor(image)\n",
    "tensor_image[None, :, :, :].shape\n",
    "tensor_image = torch.unsqueeze(tensor_image, dim=0)\n",
    "tensor_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dfd7e-c9c5-4de0-ac79-3fc3b388c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tensor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb417ab-3650-4686-9f6e-b5f7c9217004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
